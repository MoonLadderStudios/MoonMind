# Template for VLLM Service Environment Variables
# Copy this file to .env and customize as needed, or set these variables in your shell.

# The Hugging Face model identifier to be used by VLLM.
# Example: VLLM_MODEL_NAME="mistralai/Mistral-7B-Instruct-v0.1"
VLLM_MODEL_NAME="ByteDance-Seed/UI-TARS-1.5-7B"

# The data type for model weights (e.g., float16, bfloat16, auto).
# Using "auto" allows VLLM to choose the best dtype based on the model and hardware.
# Example: VLLM_DTYPE="auto"
VLLM_DTYPE="float16"

# Proportion of GPU memory to be used by VLLM (a value between 0.0 and 1.0).
# Example: VLLM_GPU_MEMORY_UTILIZATION="0.95"
VLLM_GPU_MEMORY_UTILIZATION="0.90"

# You can also specify other VLLM arguments here if needed by modifying the
# entrypoint script and passing them, for example:
# VLLM_MAX_MODEL_LEN=4096

# Codex shard and login configuration (mirrors primary .env template)
CODEX_SHARDS=3
CODEX_QUEUE=""
CODEX_VOLUME_NAME=""
CODEX_LOGIN_CHECK_IMAGE=""
# Example broker URL: "amqp://guest:<RABBITMQ_PASSWORD>@rabbitmq:5672//"
SPEC_WORKFLOW_CELERY_BROKER_URL=""
# Example result backend: "db+postgresql://postgres:<POSTGRES_PASSWORD>@api-db:5432/moonmind"
SPEC_WORKFLOW_CELERY_RESULT_BACKEND=""
SPEC_WORKFLOW_CODEX_QUEUE="codex"
SPEC_WORKFLOW_ARTIFACT_ROOT="var/artifacts/spec_workflows"

# Orchestrator service defaults
ORCHESTRATOR_CELERY_QUEUE="orchestrator.run"
ORCHESTRATOR_ARTIFACT_ROOT="var/artifacts/spec_workflows"
ORCHESTRATOR_STATSD_HOST=""
ORCHESTRATOR_STATSD_PORT=8125
ORCHESTRATOR_DOCKER_HOST="tcp://docker-proxy:2375"
