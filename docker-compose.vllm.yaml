version: '3.8'

services:
  vllm:
    image: vllm/vllm-openai:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./model_data:/root/.cache/huggingface/hub
      - ./tools/entrypoint-vllm.sh:/entrypoint-vllm.sh:ro # Added this line
    ports:
      - "8001:8000" # Exposing on 8001 to avoid conflict with existing api service on 8000
    environment:
      - MODEL_NAME=${VLLM_MODEL_NAME:-ByteDance-Seed/UI-TARS-1.5-7B}
      - DTYPE=${VLLM_DTYPE:-float16}
      - GPU_MEMORY_UTILIZATION=${VLLM_GPU_MEMORY_UTILIZATION:-0.90}
      - HF_HOME=/root/.cache/huggingface
      # Recommended for better performance with newer GPUs
      - NVIDIA_DRIVER_CAPABILITIES=all
    entrypoint: ["/bin/sh", "/entrypoint-vllm.sh"] # Changed this line
    networks:
      - local-network
    tty: true # Keep STDOUT flowing for logs
    init: true # Run an init process

networks:
  local-network:
    external: true
