"""Skeleton for planning Jira stories from high level text."""

from __future__ import annotations

import json
import logging
from typing import Any, List, Optional

from pydantic import BaseModel, Field

from moonmind.config.settings import AppSettings
from moonmind.factories.google_factory import get_google_model


class JiraStoryPlannerError(Exception):
    """Raised for errors during LLM planning or JSON parsing."""


class StoryDraft(BaseModel):
    summary: str = Field(..., description="Short summary of the story")
    description: str = Field(..., description="Detailed description")
    issue_type: str = Field(..., description="Jira issue type")
    story_points: Optional[int] = Field(None, description="Story points estimate")
    labels: List[str] = Field(default_factory=list, description="Labels to apply")


class JiraStoryPlanner:
    """Convert a plan description into Jira issues.

    Parameters
    ----------
    plan_text : str
        Text describing the overall plan or feature to implement.
    jira_project_key : str
        Key of the Jira project where issues should be created.
    dry_run : bool, optional
        If ``True``, no changes will be made in Jira. Defaults to ``True``.
    llm_model : Any, optional
        Language model used for text processing.
    logger : logging.Logger, optional
        Logger for debug and progress messages.
    **jira_kwargs : Any
        Additional arguments forwarded to the Jira client when implemented.
    """

    def __init__(
        self,
        plan_text: str,
        jira_project_key: str,
        dry_run: bool = True,
        llm_model: Optional[Any] = None,
        logger: Optional[logging.Logger] = None,
        **jira_kwargs: Any,
    ) -> None:
        self.logger = logger or logging.getLogger(__name__)

        if not plan_text:
            raise ValueError("plan_text is required")
        if not jira_project_key:
            raise ValueError("jira_project_key is required")

        self.plan_text = plan_text
        self.jira_project_key = jira_project_key
        self.dry_run = dry_run
        self.llm_model = llm_model
        self.jira_kwargs = jira_kwargs

        # Load Jira credentials from application settings
        settings = AppSettings()
        self.jira_url = settings.atlassian.atlassian_url
        self.jira_username = settings.atlassian.atlassian_username
        self.jira_api_key = settings.atlassian.atlassian_api_key

        # Placeholder for future Jira client
        self.jira_client = None

    def _build_prompt(self, plan_text: str) -> list:
        """Build LLM prompt messages from raw plan text.

        Parameters
        ----------
        plan_text : str
            The raw text describing the plan to convert into Jira stories.

        Returns
        -------
        list of Message
            A list containing the system and user messages ready for an LLM
            chat completion request.
        """
        from moonmind.schemas.chat_models import Message

        system_prompt = (
            "You are a Jira planning assistant. "
            "Return ONLY a JSON array of issues using the fields "
            "'summary', 'description', 'issue_type', 'story_points', and "
            "'labels'."
        )

        return [
            Message(role="system", content=system_prompt),
            Message(role="user", content=plan_text),
        ]

    def _call_llm(self, prompt: list) -> List[StoryDraft]:
        """Send prompt to the LLM and parse the JSON response.

        Parameters
        ----------
        prompt : list
            List of chat messages generated by ``_build_prompt``.

        Returns
        -------
        List[StoryDraft]
            Validated list of story drafts returned by the LLM.

        Raises
        ------
        JiraStoryPlannerError
            If the LLM call fails or the response is not valid JSON.
        """
        model = self.llm_model
        if model is None:
            try:
                model = get_google_model()
            except (ImportError, ValueError) as e:  # pragma: no cover - expected failure types
                self.logger.exception("Failed to initialize LLM model: %s", e)
                raise JiraStoryPlannerError(f"Failed to initialize LLM model: {e}") from e

        try:
            response = model.generate_content(prompt)
        except Exception as e:
            self.logger.exception("LLM generation error: %s", e)
            raise JiraStoryPlannerError(f"LLM generation error: {e}") from e

        response_text: Optional[str] = None
        try:
            if hasattr(response, "candidates") and response.candidates:
                first_candidate = response.candidates[0]
                if getattr(first_candidate, "content", None) and first_candidate.content.parts:
                    text_parts = [
                        part.text
                        for part in first_candidate.content.parts
                        if hasattr(part, "text") and part.text
                    ]
                    if text_parts:
                        response_text = "".join(text_parts)

            if not response_text and hasattr(response, "text"):
                response_text = response.text
        except Exception as e:  # pragma: no cover - unexpected failure structure
            self.logger.exception("Error extracting text from LLM response: %s", e)
            raise JiraStoryPlannerError(f"Invalid LLM response format: {e}") from e

        if not response_text:
            raise JiraStoryPlannerError("LLM returned no text content")

        try:
            parsed = json.loads(response_text)
        except Exception as e:
            self.logger.exception("Invalid JSON from LLM: %s", e)
            raise JiraStoryPlannerError(f"Invalid JSON from LLM: {e}") from e

        try:
            return [StoryDraft.model_validate(item) for item in parsed]
        except Exception as e:
            self.logger.exception("Story validation failed: %s", e)
            raise JiraStoryPlannerError(f"Story validation failed: {e}") from e
